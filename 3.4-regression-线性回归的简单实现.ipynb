{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''第一步：生成数据集'''\n",
    "from mxnet import autograd,nd\n",
    "\n",
    "num_inputs = 2;#特征值\n",
    "num_examples = 1000;#样本数量\n",
    "true_w = [2,-3.4] #真实权重\n",
    "true_b = 4.2 #真实偏差\n",
    "#创建样本特征值\n",
    "'''\n",
    "ramdom:几种采样方法说明\n",
    "（1）均匀分布采样   uniform\n",
    "（2）正太分布采样   normal\n",
    "（3）泊松分布采样   poisson\n",
    "\n",
    "'''\n",
    "features = nd.random.normal(scale=1,shape=(num_examples,num_inputs));\n",
    "#创建标签\n",
    "labels =  true_w[0] * features[:,0] + true_w[1] * features[:,1] + true_b;\n",
    "#加上噪声\n",
    "labels += nd.random.normal(scale=0.01,shape=labels.shape);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-1.4074123   2.6135678 ]\n",
      " [ 0.5195597   0.9168663 ]\n",
      " [ 1.0993656  -1.9022169 ]\n",
      " [ 1.3810002  -0.7629666 ]\n",
      " [ 1.2391903  -0.5411598 ]\n",
      " [-0.29296088 -0.30558026]\n",
      " [-1.085067    0.47845957]\n",
      " [-0.17266594 -1.1916859 ]\n",
      " [ 0.27309823 -0.6894807 ]\n",
      " [-0.14719068  1.5903426 ]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[-7.511107   2.0938976 12.872027   9.539722   8.525438   4.6608973\n",
      "  0.4070814  7.917271   7.0896244 -1.5003345]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "'''第二步：读取数据集'''\n",
    "#Gluon提供了data包来读取数据\n",
    "from mxnet.gluon import data as gdata;\n",
    "#数据样本的小批量大小\n",
    "batch_size = 10;\n",
    "#将训练数据的特征和标签组合\n",
    "dataset = gdata.ArrayDataset(features,labels);\n",
    "#随机load小批量\n",
    "data_iter = gdata.DataLoader(dataset,batch_size,shuffle=True);\n",
    "\n",
    "#看一个例子\n",
    "for X,y in data_iter:\n",
    "    print(X,y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''第三步：定义模型'''\n",
    "#nn是neural networks（神经网络）的缩写，这里边定义了大量神经网络的层\n",
    "from mxnet.gluon import nn;\n",
    "#在Gluon中，Sequential实例可以看作是一个串联各个层的容器\n",
    "net = nn.Sequential();\n",
    "#作为一个单层神经网络，输出层和输入层完全连接，因此叫做全连接层\n",
    "#在Gluon中，全连接层是一个Dense实例，我们定义该层输出个数为1\n",
    "net.add(nn.Dense(1))\n",
    "\n",
    "#PS:无需指定没一层输入的形状，例如线性会对的输入个数\n",
    "#当模型得到数据时，例如后边执行net(X)时，模型将自动推断每一层的输入个数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''第四步:初始化模型参数'''\n",
    "\n",
    "#在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差\n",
    "#我们使用mxnet中的init模块，用init.Normal(sigma=0.01)指定权重参数每个元素在初始化是\n",
    "#随机采样于均值为0，标准差为0.01的正态分布。偏差参数默认会初始化为零\n",
    "\n",
    "from mxnet import init;\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''第五步：定义损失函数'''\n",
    "#在Gluon中，loss模块定义了各种损失函数\n",
    "from mxnet.gluon import loss as gloss;\n",
    "#平方损失又称为L2范数损失\n",
    "loss = gloss.L2Loss();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''第六步：定义优化算法'''\n",
    "#小批量随机梯度下降sgd\n",
    "#创建一个Trainer实例，学习率指定为0.03，\n",
    "#该优化算法将用来迭代net实例所有通过add函数嵌套的层，所包含的全部参数\n",
    "#这些参数通过collect_params函数获取\n",
    "from mxnet import gluon;\n",
    "#设置一下优化算法\n",
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.03})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.034993\n",
      "epoch 2, loss: 0.000124\n",
      "epoch 3, loss: 0.000048\n"
     ]
    }
   ],
   "source": [
    "'''第七步：训练模型'''\n",
    "num_epochs = 3;#三轮全样本训练\n",
    "for epoch in range(1,num_epochs +1):\n",
    "    for X,y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X),y)\n",
    "        l.backward()\n",
    "        #上边计算出来的损失，在这里就能起到作用\n",
    "        trainer.step(batch_size)\n",
    "    #然后算出每一轮的损失\n",
    "    l = loss(net(features),labels);\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] \n",
      "[[ 1.9998659 -3.399725 ]]\n",
      "<NDArray 1x2 @cpu(0)>\n",
      "4.2 \n",
      "[4.1999354]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#最后我们比较一下学到的模型参数和真实的模型参数。\n",
    "#我们从net获得需要的层，并访问其权重weight和偏差bias\n",
    "\n",
    "dense = net[0]\n",
    "#先看下权重\n",
    "print(true_w,dense.weight.data())\n",
    "#再看下偏差\n",
    "print(true_b,dense.bias.data())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
