{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l #d2l做了一些为了方便的外部封装\n",
    "from mxnet import autograd,gluon,init,nd  #mxnet里边放了很多数学工具\n",
    "from mxnet.gluon  import data as gdata,loss as gloss,nn #数据读取，损失函数，和层gluon里边放的东西是最重要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''简洁实现'''\n",
    "\n",
    "'''提供一下训练数据'''\n",
    "#为了让出现过拟合的情况，训练数据给的特别少\n",
    "n_train,n_test,num_inputs = 20 ,100 ,200\n",
    "#设置权重和偏差,nd.ones返回数据都为1的数组\n",
    "true_w,true_b = nd.ones((num_inputs,1))*0.01 ,0.05\n",
    "#创建样本和标签\n",
    "features = nd.random.normal(shape=(n_train+n_test,num_inputs))\n",
    "#把labels组装出来，再加上噪音\n",
    "labels = nd.dot(features,true_w) + true_b;\n",
    "labels += nd.random.normal(scale=0.01,shape=labels.shape)\n",
    "#最后训练和测试截取的是行的部分\n",
    "train_features,test_features = features[:n_train,:],features[n_train:,:]\n",
    "train_labels,test_labels = labels[:n_train],labels[n_train:]\n",
    "\n",
    "'''读取数据集'''\n",
    "batch_size,num_epochs,lr = 1,100,0.003 \n",
    "#读取数据集\n",
    "train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "train_features, train_labels), batch_size, shuffle=True)\n",
    "\n",
    "'''损失函数'''\n",
    "loss = d2l.squared_loss\n",
    "\n",
    "\n",
    "'''其他都放在了这里边'''\n",
    "def fit_and_plot_gluon(wd):\n",
    "    #串联各个层的容器\n",
    "    net = nn.Sequential()\n",
    "    #添加层\n",
    "    net.add(nn.Dense(1))\n",
    "    #初始化模型参数\n",
    "    net.initialize(init.Normal(sigma=1))\n",
    "    #对权重衰减，权重名称一般是以weight结尾\n",
    "    trainer_w = gluon.Trainer(net.collect_params('.*weight'),'sgd',{'learning_rate':0.003,'wd':wd})\n",
    "    #不对偏差参数衰减，偏差名称一般以bias结尾\n",
    "    trainer_b = gluon.Trainer(net.collect_params('.*bias'),'sgd',{'learning_rate':0.003})\n",
    "        \n",
    "    train_ls,test_ls = [],[]\n",
    "    for _ in range(num_epochs):\n",
    "        #在这里进行w和b的优化\n",
    "        for X,y in train_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X),y)\n",
    "            l.backward()\n",
    "            #对两个Trainer实例分别调用step函数，从而分别 更新权重分偏差\n",
    "            #之前都是手动sgd，现在变成了step\n",
    "            trainer_w.step(batch_size)\n",
    "            trainer_b.step(batch_size)\n",
    "        #前边计算过了之后，我们就可以添加训练过程中的loss数据了\n",
    "        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())\n",
    "        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())\n",
    "    \n",
    "    #作图，把想要画的画下来\n",
    "    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',range(1, num_epochs + 1), test_ls, ['train', 'test'])\n",
    "    print('L2 norm of w:',net[0].weight.data().norm().asscalar())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''过拟合展示'''\n",
    "fit_and_plot_gluon(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''权重衰减之后'''\n",
    "fit_and_plot_gluon(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''总结：\n",
    "\n",
    "1.正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段\n",
    "2.权重衰减等价于L2范数正则化，通常会使学到的权重参数的元素接近于0\n",
    "3.权重衰减可以通过Gluon的wd超参数来指定\n",
    "4.可以定义多个Trainer实例对不同的模型参数使用不同的迭代方法\n",
    "\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
