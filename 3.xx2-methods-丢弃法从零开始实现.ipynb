{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''从零开始实现'''\n",
    "#dropout函数将以drop_prob的概率，丢弃NDArray输入X中的元素\n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd,gluon,init,nd\n",
    "from mxnet.gluon import loss as gloss,nn\n",
    "\n",
    "def dropout(X,drop_prob):\n",
    "    #Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常\n",
    "    assert 0 <= drop_prob <=1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    #这种情况下把全部元素丢弃\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    #nd.random.uniform 从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high.\n",
    "    #判断结果是如果小于keep_prob就返回1，属于keep_prob的概率，反之则返回0，是要丢弃的内容\n",
    "    mask = nd.random.uniform(0,1,X.shape) < keep_prob \n",
    "    #最后除以keep_prob，把值放大，这样可以保持均值不变\n",
    "    return mask * X /keep_prob\n",
    "\n",
    "#如果一点都不丢弃\n",
    "X = nd.arange(16).reshape(2,8)\n",
    "dropout(X,0)\n",
    "\n",
    "#如果以0.5的概率丢弃\n",
    "dropout(X,0.5)\n",
    "\n",
    "#如果全部丢弃\n",
    "dropout(X,1) #这里可以看出来zero_like是把数组元素清0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义模型参数'''\n",
    "#实验中，我们依旧使用Fashion-MNIST数据集 \n",
    "#我们将定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数是256\n",
    "num_inputs,num_outputs,num_hiddens1,num_hiddens2 = 784,10,256,256\n",
    "'''隐藏层1的参数'''\n",
    "W1= nd.random.normal(scale=0.01,shape=(num_inputs,num_hiddens1))\n",
    "#b1初始化为0了\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "'''隐藏层2的参数'''\n",
    "W2 = nd.random.normal(scale=0.01,shape=(num_hiddens1,num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "'''最终的输出'''\n",
    "W3 = nd.random.normal(scale=0.01,shape=(num_hiddens2,num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "#把参数汇总\n",
    "params = [w1,b1,w2,b2,w3,b3]\n",
    "for param in params:\n",
    "    param.attach_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义模型'''\n",
    "#(1)将全连接层和激活函数ReLU串联起来，并对每个激活函数的输出使用丢弃法\n",
    "#(2)我们可以设置各个层的丢弃概率，通常的建议是把靠近输入层的丢弃概率设的小一点,我们这里第一层是0.2，第二层是0.5\n",
    "#(3)我们可以通过is_training函数来判断运行模式是训练还是测试，并只在训练模式使用丢弃法\n",
    "drop_prob1,drop_prob2 = 0.2,0.5\n",
    "\n",
    "def net(X):\n",
    "    #如果满足这么多列，就全部显示，不满足就少显示一行\n",
    "    X = X.reshape((-1,num_inputs))\n",
    "    H1 = (nd.dot(X,W1)+b1).relu()\n",
    "    if autograd.is_training():#只在训练模型的时候使用丢弃法\n",
    "        H1 = dropout(H1,drop_prob1) #在第一层全连接之后添加丢弃层\n",
    "    H2 = (nd.dot(H1,W2)+b2).relu()\n",
    "    #在训练模型时使用丢弃法\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2,drop_prob2)#在第二层全连接之后添加丢弃层\n",
    "    #最终返回输出层\n",
    "    return nd.dot(H2,W3) + b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.3022, train acc 0.104, test acc 0.100\n",
      "epoch 2, loss 2.3017, train acc 0.108, test acc 0.100\n",
      "epoch 3, loss 2.3014, train acc 0.106, test acc 0.100\n",
      "epoch 4, loss 2.3013, train acc 0.109, test acc 0.100\n",
      "epoch 5, loss 2.3012, train acc 0.111, test acc 0.100\n"
     ]
    }
   ],
   "source": [
    "'''训练和测试模型'''\n",
    "#这部分与之前多层感知机的训练和测试类似,batch_size相当于一批批的\n",
    "num_epochs,lr,batch_size = 5,0.5,256\n",
    "#交叉熵损失函数SoftmaxCrossEntropyLoss()\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "#测试数据和训练数据，我们用的是封装之后的\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "#最后开始训练\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
