{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''读取数据集'''\n",
    "batch_size = 256\n",
    "#用了d2lzh中内置的读取方法\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义模型参数'''\n",
    "#Fashion-MNIST数据集中图像形状为28*28，类别数为10\n",
    "#本节依然使用长度为28*28=784的向量表示每一张图像\n",
    "#因此输入个数为784，输出个数为10，\n",
    "#实验中，我们设置隐藏单元个数为256，它是超参数\n",
    "\n",
    "num_inputs, num_outputs,num_hiddens = 784,10,256\n",
    "#隐藏层数输出层的输入，从矩阵乘法乘的法则，W1中的行对应着输入个数，num_hiddens对应着输出\n",
    "#从矩阵的角度来讲，W1对应着Ax=b中的A\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))\n",
    "b1 = nd.zeros( num_hiddens)#跟着输出的个数\n",
    "\n",
    "#隐藏层到输出层\n",
    "W2 = nd.random.normal(scale=0.01,shape=(num_hiddens,num_outputs));\n",
    "b2 = nd.zeros(num_outputs)\n",
    "\n",
    "params = {W1,b1,W2,b2}\n",
    "\n",
    "#给模型参数附上梯度空间\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义激活函数'''\n",
    "#我们使用基础的maximum函数来实现ReLU,而非直接调用MXNet的relu函数\n",
    "def relu(X):\n",
    "    return nd.maximum(X,0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义模型'''\n",
    "\n",
    "def net(X):\n",
    "    #尽量显示这个多列，如果剩下的内容不够显示一行，那么就少一行\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    #得到隐藏层的内容，通过激活函数确定一下层级\n",
    "    H = relu(nd.dot(X, W1) + b1)\n",
    "    #得到输出层的内容\n",
    "    return nd.dot(H, W2) + b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义损失函数'''\n",
    "#为了更好的数据稳定性，我们直接使用Gluon提供的函数\n",
    "#此函数包括softmax运算和交叉熵损失计算的函数\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.8498, train acc 0.687, test acc 0.817\n",
      "epoch 2, loss 0.4921, train acc 0.818, test acc 0.845\n",
      "epoch 3, loss 0.4296, train acc 0.841, test acc 0.850\n",
      "epoch 4, loss 0.3971, train acc 0.855, test acc 0.845\n",
      "epoch 5, loss 0.3800, train acc 0.860, test acc 0.871\n"
     ]
    }
   ],
   "source": [
    "'''训练模型'''\n",
    "#直接使用d2lzh包中的train_ch3函数，我们这里设置迭代周期为5，学习率为0.5\n",
    "\n",
    "num_epochs,lr = 5,0.5\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
